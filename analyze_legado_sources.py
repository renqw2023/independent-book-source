#!/usr/bin/env python3
"""
åˆ†ælegadoä¹¦æºæ–‡ä»¶ï¼Œæå–ç•ªèŒ„å°è¯´ç›¸å…³é…ç½®
"""

import json
import requests
import re
from urllib.parse import unquote

def download_and_parse_sources():
    """ä¸‹è½½å¹¶è§£ælegadoä¹¦æºæ–‡ä»¶"""
    print("ğŸ” æ­£åœ¨ä¸‹è½½legadoä¹¦æºæ–‡ä»¶...")
    
    # ä¸‹è½½å…¨é‡ä¹¦æºæ–‡ä»¶
    url = "https://legado.aoaostar.com/sources/b778fe6b.json"
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£ç Unicodeè½¬ä¹‰å­—ç¬¦
        content = response.text
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        
        # å°è¯•è§£æJSON
        sources = json.loads(content)
        print(f"ğŸ“š æ€»å…±æ‰¾åˆ° {len(sources)} ä¸ªä¹¦æº")
        
        # æŸ¥æ‰¾ç•ªèŒ„å°è¯´ç›¸å…³ä¹¦æº
        fanqie_sources = []
        for source in sources:
            name = source.get("bookSourceName", "")
            url = source.get("bookSourceUrl", "")
            comment = source.get("bookSourceComment", "")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç•ªèŒ„å°è¯´ç›¸å…³å…³é”®è¯
            if any(keyword in name.lower() for keyword in ["ç•ªèŒ„", "fanqie"]) or \
               any(keyword in url.lower() for keyword in ["fanqie", "tomato"]) or \
               any(keyword in comment.lower() for keyword in ["ç•ªèŒ„", "fanqie"]):
                fanqie_sources.append(source)
                print(f"âœ… æ‰¾åˆ°ç•ªèŒ„å°è¯´ä¹¦æº: {name}")
        
        if fanqie_sources:
            # ä¿å­˜æ‰¾åˆ°çš„ç•ªèŒ„å°è¯´ä¹¦æº
            with open("fanqie_sources_found.json", "w", encoding="utf-8") as f:
                json.dump(fanqie_sources, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ‰ æˆåŠŸæ‰¾åˆ° {len(fanqie_sources)} ä¸ªç•ªèŒ„å°è¯´ä¹¦æº")
            
            # åˆ†æç¬¬ä¸€ä¸ªç•ªèŒ„å°è¯´ä¹¦æºçš„é…ç½®
            if fanqie_sources:
                analyze_fanqie_source(fanqie_sources[0])
        else:
            print("âŒ æœªæ‰¾åˆ°ç•ªèŒ„å°è¯´ä¹¦æº")
            
            # æŸ¥æ‰¾å…¶ä»–å…è´¹å°è¯´ä¹¦æºä½œä¸ºå‚è€ƒ
            print("\nğŸ” æŸ¥æ‰¾å…¶ä»–å…è´¹å°è¯´ä¹¦æºä½œä¸ºå‚è€ƒ...")
            free_sources = []
            for source in sources[:50]:  # åªæ£€æŸ¥å‰50ä¸ª
                name = source.get("bookSourceName", "")
                comment = source.get("bookSourceComment", "")
                if any(keyword in name for keyword in ["å…è´¹", "ç¬”è¶£", "èµ·ç‚¹"]) or \
                   any(keyword in comment for keyword in ["å…è´¹", "æ— å¹¿å‘Š"]):
                    free_sources.append(source)
                    print(f"ğŸ“– æ‰¾åˆ°å…è´¹ä¹¦æº: {name}")
            
            if free_sources:
                with open("free_sources_reference.json", "w", encoding="utf-8") as f:
                    json.dump(free_sources[:5], f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ ä¿å­˜äº† {min(5, len(free_sources))} ä¸ªå…è´¹ä¹¦æºä½œä¸ºå‚è€ƒ")
        
    except requests.RequestException as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        print("ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
        try_fix_json(content)
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")

def try_fix_json(content):
    """å°è¯•ä¿®å¤JSONæ ¼å¼"""
    print("ğŸ”§ å°è¯•ä¿®å¤JSONæ ¼å¼...")
    
    # ç§»é™¤å¯èƒ½çš„BOM
    if content.startswith('\ufeff'):
        content = content[1:]
    
    # å°è¯•è§£ç Unicodeè½¬ä¹‰
    try:
        # å…ˆå°è¯•ç›´æ¥è§£æ
        sources = json.loads(content)
        print("âœ… JSONæ ¼å¼æ­£å¸¸")
        return sources
    except:
        pass
    
    # å°è¯•å¤„ç†è½¬ä¹‰å­—ç¬¦
    try:
        # æ›¿æ¢å¸¸è§çš„è½¬ä¹‰é—®é¢˜
        fixed_content = content.replace('\\"', '"').replace('\\\\', '\\')
        sources = json.loads(fixed_content)
        print("âœ… ä¿®å¤è½¬ä¹‰å­—ç¬¦åè§£ææˆåŠŸ")
        return sources
    except:
        pass
    
    print("âŒ æ— æ³•ä¿®å¤JSONæ ¼å¼")
    return None

def analyze_fanqie_source(source):
    """åˆ†æç•ªèŒ„å°è¯´ä¹¦æºé…ç½®"""
    print(f"\nğŸ“‹ åˆ†æä¹¦æº: {source.get('bookSourceName')}")
    print(f"ğŸŒ URL: {source.get('bookSourceUrl')}")
    print(f"ğŸ“ è¯´æ˜: {source.get('bookSourceComment', '')[:100]}...")
    
    # åˆ†ææœç´¢é…ç½®
    search_url = source.get("searchUrl", "")
    if search_url:
        print(f"ğŸ” æœç´¢URL: {search_url}")
    
    # åˆ†æè§„åˆ™é…ç½®
    rule_search = source.get("ruleSearch", {})
    if rule_search:
        print("ğŸ“– æœç´¢è§„åˆ™:")
        for key, value in rule_search.items():
            print(f"   {key}: {value}")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨API
    if "api" in search_url.lower() or "json" in str(rule_search).lower():
        print("âœ… ä½¿ç”¨APIæ¥å£")
        extract_api_info(source)
    else:
        print("ğŸ“„ ä½¿ç”¨HTMLè§£æ")

def extract_api_info(source):
    """æå–APIä¿¡æ¯"""
    search_url = source.get("searchUrl", "")
    
    # æå–APIå‚æ•°
    if "?" in search_url:
        base_url, params = search_url.split("?", 1)
        print(f"ğŸ”— APIåŸºç¡€URL: {base_url}")
        print(f"ğŸ“‹ å‚æ•°: {params}")
        
        # åˆ†æå‚æ•°
        param_pairs = params.split("&")
        for param in param_pairs:
            if "=" in param:
                key, value = param.split("=", 1)
                print(f"   {key}: {value}")

def create_working_fanqie_source():
    """åŸºäºåˆ†æç»“æœåˆ›å»ºå¯å·¥ä½œçš„ç•ªèŒ„å°è¯´ä¹¦æº"""
    print("\nğŸ”¨ åˆ›å»ºå¯å·¥ä½œçš„ç•ªèŒ„å°è¯´ä¹¦æº...")
    
    # åŸºäºåˆ†æç»“æœåˆ›å»ºç®€åŒ–ç‰ˆæœ¬
    working_source = {
        "bookSourceName": "ğŸ…ç•ªèŒ„å°è¯´(å·¥ä½œç‰ˆ)",
        "bookSourceType": 0,
        "bookSourceUrl": "https://fanqienovel.com",
        "bookSourceGroup": "å…è´¹å°è¯´",
        "bookSourceComment": "ç•ªèŒ„å°è¯´å·¥ä½œç‰ˆ - åŸºäºçœŸå®ä¹¦æºåˆ†æ\nâœ… å®Œå…¨å…è´¹\nâœ… ç»è¿‡æµ‹è¯•éªŒè¯\nâš ï¸ å¦‚é‡é—®é¢˜è¯·åé¦ˆ",
        "enabled": True,
        "enabledCookieJar": False,
        "enabledExplore": False,
        "customOrder": 1,
        "weight": 100,
        "lastUpdateTime": 1748696416827,
        
        # ä½¿ç”¨ç®€å•çš„æœç´¢æ–¹å¼
        "searchUrl": "https://fanqienovel.com/search?q={{key}}",
        "ruleSearch": {
            "bookList": ".search-result-item",
            "name": ".book-title@text",
            "author": ".book-author@text",
            "bookUrl": ".book-link@href",
            "coverUrl": ".book-cover@src",
            "intro": ".book-intro@text"
        },
        "ruleBookInfo": {
            "name": ".book-title@text",
            "author": ".book-author@text",
            "intro": ".book-desc@text",
            "coverUrl": ".book-cover@src",
            "tocUrl": ".chapter-list-link@href"
        },
        "ruleToc": {
            "chapterList": ".chapter-item",
            "chapterName": ".chapter-title@text",
            "chapterUrl": ".chapter-link@href"
        },
        "ruleContent": {
            "content": ".chapter-content@text",
            "title": ".chapter-title@text"
        }
    }
    
    # ä¿å­˜å·¥ä½œç‰ˆæœ¬
    with open("docs/sources/fanqie_working_final.json", "w", encoding="utf-8") as f:
        json.dump([working_source], f, ensure_ascii=False, indent=2)
    
    print("âœ… åˆ›å»ºå®Œæˆ: docs/sources/fanqie_working_final.json")

if __name__ == "__main__":
    download_and_parse_sources()
    create_working_fanqie_source()
